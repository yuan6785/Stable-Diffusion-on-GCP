FROM nvidia/cuda:11.8.0-runtime-ubuntu22.04

# apt install -y wget git python3 python3-venv python3-pip xdg-utils && \
# xdg-utils 这个包太大了，要安装很久，取消掉，是一个远程桌面系统的包，用不到
# /usr/bin/supervisord -c /etc/supervisor.conf # 手动启动supervisor /usr/bin/supervisorctl stop all;kill pid  # 停止所有程序并杀掉supervisor进程
RUN set -ex && \
    apt update && \
    apt install -y wget git libglib2.0-dev && \
    rm -rf /var/lib/apt/lists/*

ENV LD_LIBRARY_PATH $LD_LIBRARY_PATH:/usr/local/cuda/lib64
RUN ln -s /usr/local/cuda/lib64/libcudart.so.11.0 /usr/local/cuda/lib64/libcudart.so

# 
WORKDIR /root
#
RUN ls -l /root
RUN wget https://repo.anaconda.com/miniconda/Miniconda3-py38_23.1.0-1-Linux-x86_64.sh
RUN chmod -Rf 777 Miniconda3-py38_23.1.0-1-Linux-x86_64.sh&&chmod +x Miniconda3-py38_23.1.0-1-Linux-x86_64.sh
# /root/miniconda3/envs/sd_python310/bin/python -m pip install virtualenv 或者 /root/miniconda3/bin/conda install -n sd_python310 virtualenv
# && /root/miniconda3/envs/sd_python310/bin/python  launch_gcp.py --skip-torch-cuda-test  \ 原来用这个，后来改为python -c的方式
# 使用 --skip-torch-cuda-test, 因为cloudbuild.yaml的machineType是没有GPU的； 另一种思路，取消这个参数，但是cloudbuild.yaml的machineType改成有GPU的也可以尝试下；----可以用打好的镜像创建一个ubuntu的pod参考deployment_ubuntu_sd_test.yaml进行调试
RUN set -ex \
    && bash Miniconda3-py38_23.1.0-1-Linux-x86_64.sh -b -p /root/miniconda3 \
    && /root/miniconda3/bin/conda init \
    && /root/miniconda3/bin/conda create -y --name sd_python310 python=3.10 \
    && /root/miniconda3/bin/conda install -y -n sd_python310 virtualenv \
    && git clone https://github.com/yuan6785/stable-diffusion-webui.git \
    && cd stable-diffusion-webui \
    && git checkout b791502f18b0de3e9eb93fca3aadce42878904d8 \
    && /root/miniconda3/envs/sd_python310/bin/python -c "import launch;launch.prepare_environment();" --skip-torch-cuda-test \
    && cd extensions \
    && git clone https://github.com/Mikubill/sd-webui-controlnet.git && cd sd-webui-controlnet && git checkout eff5fb5c0e9ee3c5d0fc9a3c5ec0c10fce8ee3cd && cd .. \
    && git clone https://github.com/KohakuBlueleaf/a1111-sd-webui-locon.git && cd a1111-sd-webui-locon && git checkout 04b768bad41e0f1122a86d0636fce47bb2765af8 && cd .. \
    && git clone https://github.com/DominikDoom/a1111-sd-webui-tagcomplete.git && cd a1111-sd-webui-tagcomplete && git checkout 2.2.0 && cd .. \
    && git clone https://github.com/fkunn1326/openpose-editor && cd openpose-editor && git checkout 0b10737e3a1226bb13346aa82b61ebb62c230b41 && cd .. \
    && git clone https://github.com/hnmr293/posex && cd posex && git checkout 61169d30ae1be889b04d873522d9b985b64405bb && cd ..\
    && git clone https://github.com/kohya-ss/sd-webui-additional-networks && cd sd-webui-additional-networks && git checkout v0.5.0 && cd .. \
    && git clone https://github.com/jexom/sd-webui-depth-lib.git && cd sd-webui-depth-lib && git checkout efa9f616b30ea0f27e5dadf40ab41815012d1d78 && cd .. \
    && git clone https://github.com/Coyote-A/ultimate-upscale-for-automatic1111.git && cd ultimate-upscale-for-automatic1111 && git checkout 0a3d03a41aef6f0a2b7f9582e42e06852881b4a8 && cd ..

COPY docker_data/config.json /root/stable-diffusion-webui/config.json

# Install TensorRT and set cuda env, 这一步其实可以不要，但按原作者的思想，加进去吧
RUN set -ex \
    && /root/miniconda3/envs/sd_python310/bin/python -m pip install tensorrt==8.6.0
ARG CUDNN_PATH=$(dirname $(/root/miniconda3/envs/sd_python310/bin/python -c "import nvidia.cudnn;print(nvidia.cudnn.__file__)"))
ARG TENSORRT_PATH=$(dirname $(/root/miniconda3/envs/sd_python310/bin/python -c "import tensorrt;print(tensorrt.__file__)"))
ENV LD_LIBRARY_PATH $LD_LIBRARY_PATH:/usr/local/cuda/lib64:$CUDNN_PATH:$TENSORRT_PATH
#
EXPOSE 7860

WORKDIR /root/stable-diffusion-webui/
# # /root/miniconda3/envs/sd_python310/bin/python launch.py --listen --xformers --medvram --share
CMD ["/root/miniconda3/envs/sd_python310/bin/python", "launch.py", "--listen", "--xformers",  "--medvram"]