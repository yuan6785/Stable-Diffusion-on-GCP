{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/scripts/gcp-stable-diffusion-build-deploy/Training'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "# TODO\n",
    "# 1. adjust arguments according to available VRAM\n",
    "# 2. adjust arguments according to instance images number/person/object training\n",
    "# 3. adjust arguments according to input folder/file type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(input_args=None):\n",
    "    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n",
    "    parser.add_argument(\n",
    "        \"--pretrained_model_name_or_path\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--models_dir\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"Path to dump the trained model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--revision\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=False,\n",
    "        help=(\n",
    "            \"Revision of pretrained model identifier from huggingface.co/models. Trainable model components should be\"\n",
    "            \" float32 precision.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tokenizer_name\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--instance_data_dir\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"A folder containing the training data of instance images.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--class_data_dir\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=False,\n",
    "        help=\"A folder containing the training data of class images.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--instance_prompt\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"The prompt with identifier specifying the instance\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--class_prompt\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The prompt to specify images in the same class as provided instance images.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--with_prior_preservation\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Flag to add prior preservation loss.\",\n",
    "    )\n",
    "    parser.add_argument(\"--prior_loss_weight\", type=float, default=1.0, help=\"The weight of prior preservation loss.\")\n",
    "    parser.add_argument(\n",
    "        \"--num_class_images\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help=(\n",
    "            \"Minimal class images for prior preservation loss. If there are not enough images already present in\"\n",
    "            \" class_data_dir, additional images will be sampled with class_prompt.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        type=str,\n",
    "        default=\"text-inversion-model\",\n",
    "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", type=int, default=None, help=\"A seed for reproducible training.\")\n",
    "    parser.add_argument(\n",
    "        \"--resolution\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=(\n",
    "            \"The resolution for input images, all the images in the train/validation dataset will be resized to this\"\n",
    "            \" resolution\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--center_crop\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=(\n",
    "            \"Whether to center crop the input images to the resolution. If not set, the images will be randomly\"\n",
    "            \" cropped. The images will be resized to the resolution first before cropping.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_text_encoder\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to train the text encoder. If set, the text encoder should be float32 precision.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\", type=int, default=4, help=\"Batch size (per device) for the training dataloader.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sample_batch_size\", type=int, default=4, help=\"Batch size (per device) for sampling images.\"\n",
    "    )\n",
    "    parser.add_argument(\"--num_train_epochs\", type=int, default=1)\n",
    "    parser.add_argument(\n",
    "        \"--max_train_steps\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpointing_steps\",\n",
    "        type=int,\n",
    "        default=500,\n",
    "        help=(\n",
    "            \"Save a checkpoint of the training state every X updates. Checkpoints can be used for resuming training via `--resume_from_checkpoint`. \"\n",
    "            \"In the case that the checkpoint is better than the final trained model, the checkpoint can also be used for inference.\"\n",
    "            \"Using a checkpoint for inference requires separate loading of the original pipeline and the individual checkpointed model components.\"\n",
    "            \"See https://huggingface.co/docs/diffusers/main/en/training/dreambooth#performing-inference-using-a-saved-checkpoint for step by step\"\n",
    "            \"instructions.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpoints_total_limit\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=(\n",
    "            \"Max number of checkpoints to store. Passed as `total_limit` to the `Accelerator` `ProjectConfiguration`.\"\n",
    "            \" See Accelerator::save_state https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator.save_state\"\n",
    "            \" for more details\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_from_checkpoint\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=(\n",
    "            \"Whether training should be resumed from a previous checkpoint. Use a path saved by\"\n",
    "            ' `--checkpointing_steps`, or `\"latest\"` to automatically select the last available checkpoint.'\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_checkpointing\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=float,\n",
    "        default=5e-6,\n",
    "        help=\"Initial learning rate (after the potential warmup period) to use.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--scale_lr\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_scheduler\",\n",
    "        type=str,\n",
    "        default=\"constant\",\n",
    "        help=(\n",
    "            'The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\",'\n",
    "            ' \"constant\", \"constant_with_warmup\"]'\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_warmup_steps\", type=int, default=500, help=\"Number of steps for the warmup in the lr scheduler.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_num_cycles\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of hard resets of the lr in cosine_with_restarts scheduler.\",\n",
    "    )\n",
    "    parser.add_argument(\"--lr_power\", type=float, default=1.0, help=\"Power factor of the polynomial scheduler.\")\n",
    "    parser.add_argument(\n",
    "        \"--use_8bit_adam\", action=\"store_true\", help=\"Whether or not to use 8-bit Adam from bitsandbytes.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataloader_num_workers\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=(\n",
    "            \"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\"--adam_beta1\", type=float, default=0.9, help=\"The beta1 parameter for the Adam optimizer.\")\n",
    "    parser.add_argument(\"--adam_beta2\", type=float, default=0.999, help=\"The beta2 parameter for the Adam optimizer.\")\n",
    "    parser.add_argument(\"--adam_weight_decay\", type=float, default=1e-2, help=\"Weight decay to use.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", type=float, default=1e-08, help=\"Epsilon value for the Adam optimizer\")\n",
    "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the model to the Hub.\")\n",
    "    parser.add_argument(\"--hub_token\", type=str, default=None, help=\"The token to use to push to the Model Hub.\")\n",
    "    parser.add_argument(\n",
    "        \"--hub_model_id\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The name of the repository to keep in sync with the local `output_dir`.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--logging_dir\",\n",
    "        type=str,\n",
    "        default=\"logs\",\n",
    "        help=(\n",
    "            \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\"\n",
    "            \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--allow_tf32\",\n",
    "        action=\"store_true\",\n",
    "        help=(\n",
    "            \"Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see\"\n",
    "            \" https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--report_to\",\n",
    "        type=str,\n",
    "        default=\"tensorboard\",\n",
    "        help=(\n",
    "            'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`'\n",
    "            ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.'\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--validation_prompt\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"A prompt that is used during validation to verify that the model is learning.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_validation_images\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help=\"Number of images that should be generated during validation with `validation_prompt`.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--validation_steps\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help=(\n",
    "            \"Run validation every X steps. Validation consists of running the prompt\"\n",
    "            \" `args.validation_prompt` multiple times: `args.num_validation_images`\"\n",
    "            \" and logging the images.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mixed_precision\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        choices=[\"no\", \"fp16\", \"bf16\"],\n",
    "        help=(\n",
    "            \"Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >=\"\n",
    "            \" 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the\"\n",
    "            \" flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--prior_generation_precision\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        choices=[\"no\", \"fp32\", \"fp16\", \"bf16\"],\n",
    "        help=(\n",
    "            \"Choose prior generation precision between fp32, fp16 and bf16 (bfloat16). Bf16 requires PyTorch >=\"\n",
    "            \" 1.10.and an Nvidia Ampere GPU.  Default to  fp16 if a GPU is available else fp32.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
    "    parser.add_argument(\n",
    "        \"--enable_xformers_memory_efficient_attention\", action=\"store_true\", help=\"Whether or not to use xformers.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--set_grads_to_none\",\n",
    "        action=\"store_true\",\n",
    "        help=(\n",
    "            \"Save more memory by using setting grads to None instead of zero. Be aware, that this changes certain\"\n",
    "            \" behaviors, so disable this argument if it causes any problems. More info:\"\n",
    "            \" https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html\"\n",
    "        ),\n",
    "    )\n",
    "    #\n",
    "    parser.add_argument(\n",
    "        \"--auto_guess\",\n",
    "        action=\"store_true\",\n",
    "        help=(\n",
    "            \"Auto guess the best parameters according to input image numbers, available vram, and more\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    if input_args is not None:\n",
    "        args = parser.parse_args(input_args)\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    if env_local_rank != -1 and env_local_rank != args.local_rank:\n",
    "        args.local_rank = env_local_rank\n",
    "\n",
    "    if args.with_prior_preservation:\n",
    "        if args.class_data_dir is None:\n",
    "            raise ValueError(\"You must specify a data directory for class images.\")\n",
    "        if args.class_prompt is None:\n",
    "            raise ValueError(\"You must specify prompt for class images.\")\n",
    "    else:\n",
    "        # logger is not available yet\n",
    "        if args.class_data_dir is not None:\n",
    "            warnings.warn(\"You need not use --class_data_dir without --with_prior_preservation.\")\n",
    "        if args.class_prompt is not None:\n",
    "            warnings.warn(\"You need not use --class_prompt without --with_prior_preservation.\")\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_args = [\n",
    "  '--pretrained_model_name_or_path=./realisticVisionV13.LtFu.safetensors',\n",
    "  '--instance_data_dir=/mnt/vol1/inputs/alvan-nee-cropped/',\n",
    "  '--class_data_dir=./dog_class',\n",
    "  '--output_dir=./mydog_real',\n",
    "  '--with_prior_preservation',\n",
    "  '--prior_loss_weight=1.0',\n",
    "  '--train_text_encoder',\n",
    "  '--instance_prompt=a photo of my dog',\n",
    "  '--class_prompt=a photo of a dog',\n",
    "  '--resolution=512',\n",
    "  '--train_batch_size=1',\n",
    "  '--mixed_precision=fp16',\n",
    "  '--use_8bit_adam',\n",
    "  '--gradient_accumulation_steps=1',\n",
    "  '--gradient_checkpointing',\n",
    "  '--learning_rate=1e-6',\n",
    "  '--lr_scheduler=constant',\n",
    "  '--lr_warmup_steps=200',\n",
    "  '--num_class_images=300',\n",
    "  '--max_train_steps=1500',\n",
    "  '--checkpointing_steps=300',\n",
    "  '--checkpoints_total_limit=6',\n",
    "  '--enable_xformers_memory_efficient_attention',\n",
    "  '--auto_guess',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--controlnet'], dest='controlnet', nargs=0, const=True, default=None, type=None, choices=None, required=False, help='Set flag if this is a controlnet checkpoint.', metavar=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers.pipelines.stable_diffusion.convert_from_ckpt import download_from_original_stable_diffusion_ckpt\n",
    "\n",
    "sd_to_diff_parser = argparse.ArgumentParser()\n",
    "\n",
    "sd_to_diff_parser.add_argument(\n",
    "    \"--checkpoint_path\", default=None, type=str, required=True, help=\"Path to the checkpoint to convert.\"\n",
    ")\n",
    "# !wget https://raw.githubusercontent.com/CompVis/stable-diffusion/main/configs/stable-diffusion/v1-inference.yaml\n",
    "sd_to_diff_parser.add_argument(\n",
    "    \"--original_config_file\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    help=\"The YAML config file corresponding to the original architecture.\",\n",
    ")\n",
    "sd_to_diff_parser.add_argument(\n",
    "    \"--num_in_channels\",\n",
    "    default=None,\n",
    "    type=int,\n",
    "    help=\"The number of input channels. If `None` number of input channels will be automatically inferred.\",\n",
    ")\n",
    "sd_to_diff_parser.add_argument(\n",
    "    \"--scheduler_type\",\n",
    "    default=\"pndm\",\n",
    "    type=str,\n",
    "    help=\"Type of scheduler to use. Should be one of ['pndm', 'lms', 'ddim', 'euler', 'euler-ancestral', 'dpm']\",\n",
    ")\n",
    "sd_to_diff_parser.add_argument(\n",
    "    \"--pipeline_type\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    help=(\n",
    "        \"The pipeline type. One of 'FrozenOpenCLIPEmbedder', 'FrozenCLIPEmbedder', 'PaintByExample'\"\n",
    "        \". If `None` pipeline will be automatically inferred.\"\n",
    "    ),\n",
    ")\n",
    "sd_to_diff_parser.add_argument(\n",
    "    \"--image_size\",\n",
    "    default=None,\n",
    "    type=int,\n",
    "    help=(\n",
    "        \"The image size that the model was trained on. Use 512 for Stable Diffusion v1.X and Stable Siffusion v2\"\n",
    "        \" Base. Use 768 for Stable Diffusion v2.\"\n",
    "    ),\n",
    ")\n",
    "sd_to_diff_parser.add_argument(\n",
    "    \"--prediction_type\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    help=(\n",
    "        \"The prediction type that the model was trained on. Use 'epsilon' for Stable Diffusion v1.X and Stable\"\n",
    "        \" Diffusion v2 Base. Use 'v_prediction' for Stable Diffusion v2.\"\n",
    "    ),\n",
    ")\n",
    "sd_to_diff_parser.add_argument(\n",
    "    \"--extract_ema\",\n",
    "    action=\"store_true\",\n",
    "    help=(\n",
    "        \"Only relevant for checkpoints that have both EMA and non-EMA weights. Whether to extract the EMA weights\"\n",
    "        \" or not. Defaults to `False`. Add `--extract_ema` to extract the EMA weights. EMA weights usually yield\"\n",
    "        \" higher quality images for inference. Non-EMA weights are usually better to continue fine-tuning.\"\n",
    "    ),\n",
    ")\n",
    "sd_to_diff_parser.add_argument(\n",
    "    \"--upcast_attention\",\n",
    "    action=\"store_true\",\n",
    "    help=(\n",
    "        \"Whether the attention computation should always be upcasted. This is necessary when running stable\"\n",
    "        \" diffusion 2.1.\"\n",
    "    ),\n",
    ")\n",
    "sd_to_diff_parser.add_argument(\n",
    "    \"--from_safetensors\",\n",
    "    action=\"store_true\",\n",
    "    help=\"If `--checkpoint_path` is in `safetensors` format, load checkpoint with safetensors instead of PyTorch.\",\n",
    ")\n",
    "sd_to_diff_parser.add_argument(\n",
    "    \"--to_safetensors\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether to store pipeline in safetensors format or not.\",\n",
    ")\n",
    "sd_to_diff_parser.add_argument(\"--dump_path\", default=None, type=str, required=True, help=\"Path to the output model.\")\n",
    "sd_to_diff_parser.add_argument(\"--device\", type=str, help=\"Device to use (e.g. cpu, cuda:0, cuda:1, etc.)\")\n",
    "sd_to_diff_parser.add_argument(\n",
    "    \"--stable_unclip\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    required=False,\n",
    "    help=\"Set if this is a stable unCLIP model. One of 'txt2img' or 'img2img'.\",\n",
    ")\n",
    "sd_to_diff_parser.add_argument(\n",
    "    \"--stable_unclip_prior\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    required=False,\n",
    "    help=\"Set if this is a stable unCLIP txt2img model. Selects which prior to use. If `--stable_unclip` is set to `txt2img`, the karlo prior (https://huggingface.co/kakaobrain/karlo-v1-alpha/tree/main/prior) is selected by default.\",\n",
    ")\n",
    "sd_to_diff_parser.add_argument(\n",
    "    \"--clip_stats_path\",\n",
    "    type=str,\n",
    "    help=\"Path to the clip stats file. Only required if the stable unclip model's config specifies `model.params.noise_aug_config.params.clip_stats_path`.\",\n",
    "    required=False,\n",
    ")\n",
    "sd_to_diff_parser.add_argument(\n",
    "    \"--controlnet\", action=\"store_true\", default=None, help=\"Set flag if this is a controlnet checkpoint.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args(input_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./realisticVisionV13.LtFu.safetensors'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.pretrained_model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "basename = None\n",
    "if os.path.isfile(args.pretrained_model_name_or_path):\n",
    "    file = args.pretrained_model_name_or_path\n",
    "    basename = file.split('/')[-1].split('.')[0]\n",
    "    file_extension = file.split('/')[-1].split('.')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'realisticVisionV13'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step key not found in model\n",
      "In this conversion only the non-EMA weights are extracted. If you want to instead extract the EMA weights (usually better for inference), please make sure to add the `--extract_ema` flag.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'logit_scale', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/hzchen/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if basename:\n",
    "  sd_to_diff_parser_input_args = [\n",
    "    '--checkpoint_path={}'.format(args.pretrained_model_name_or_path),\n",
    "    '--dump_path={}'.format(basename),\n",
    "  ]\n",
    "  if file_extension == 'safetensors':\n",
    "    sd_to_diff_parser_input_args.append('--from_safetensors')\n",
    "  if args.resolution:\n",
    "    sd_to_diff_parser_input_args.append('--image_size={}'.format(args.resolution))\n",
    "  if os.path.isfile(basename + '.yaml'):\n",
    "    sd_to_diff_parser_input_args.append('--original_config_file={}'.format(basename + '.yaml'))\n",
    "    \n",
    "  sd_to_diff_args = sd_to_diff_parser.parse_args(sd_to_diff_parser_input_args)\n",
    "  \n",
    "  pipe = download_from_original_stable_diffusion_ckpt(\n",
    "    checkpoint_path=sd_to_diff_args.checkpoint_path,\n",
    "    original_config_file=sd_to_diff_args.original_config_file,\n",
    "    image_size=sd_to_diff_args.image_size,\n",
    "    prediction_type=sd_to_diff_args.prediction_type,\n",
    "    model_type=sd_to_diff_args.pipeline_type,\n",
    "    extract_ema=sd_to_diff_args.extract_ema,\n",
    "    scheduler_type=sd_to_diff_args.scheduler_type,\n",
    "    num_in_channels=sd_to_diff_args.num_in_channels,\n",
    "    upcast_attention=sd_to_diff_args.upcast_attention,\n",
    "    from_safetensors=sd_to_diff_args.from_safetensors,\n",
    "    device=sd_to_diff_args.device,\n",
    "    stable_unclip=sd_to_diff_args.stable_unclip,\n",
    "    stable_unclip_prior=sd_to_diff_args.stable_unclip_prior,\n",
    "    clip_stats_path=sd_to_diff_args.clip_stats_path,\n",
    "    controlnet=sd_to_diff_args.controlnet,\n",
    "  )\n",
    "\n",
    "  if sd_to_diff_args.controlnet:\n",
    "      # only save the controlnet model\n",
    "      pipe.controlnet.save_pretrained(sd_to_diff_args.dump_path, safe_serialization=sd_to_diff_args.to_safetensors)\n",
    "  else:\n",
    "      pipe.save_pretrained(sd_to_diff_args.dump_path, safe_serialization=sd_to_diff_args.to_safetensors)\n",
    "      \n",
    "  args.pretrained_model_name_or_path = sd_to_diff_args.dump_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.auto_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.train_text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvidia_smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 15.0\n",
      "Free memory: 14.56085205078125\n",
      "Used memory: 0.43914794921875\n"
     ]
    }
   ],
   "source": [
    "nvidia_smi.nvmlInit()\n",
    "\n",
    "handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "# card id 0 hardcoded here, there is also a call to get all available card ids, so we could iterate\n",
    "\n",
    "gpu_info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "\n",
    "print(\"Total memory:\", gpu_info.total/1024/1024/1024)\n",
    "print(\"Free memory:\", gpu_info.free/1024/1024/1024)\n",
    "print(\"Used memory:\", gpu_info.used/1024/1024/1024)\n",
    "\n",
    "nvidia_smi.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.auto_guess:\n",
    "    args.with_prior_preservation = True\n",
    "    args.prior_loss_weight = 1.0\n",
    "    args.train_text_encoder = True\n",
    "    args.learning_rate = 1e-06\n",
    "    args.lr_scheduler = \"polynomial\"\n",
    "    num_instance_images = len(glob(os.path.join(args.instance_data_dir, '*')))\n",
    "    args.num_class_images = num_instance_images * 12\n",
    "    args.max_train_steps = min(int(num_instance_images * 80 * 1.25), 1500)\n",
    "    args.lr_warmup_steps = args.max_train_steps // 10\n",
    "    args.checkpoints_total_limit = 5\n",
    "    args.checkpointing_steps = args.max_train_steps // args.checkpoints_total_limit\n",
    "    args.mixed_precision = \"fp16\"\n",
    "    args.use_8bit_adam = True\n",
    "    args.train_batch_size = 2\n",
    "    args.enable_xformers_memory_efficient_attention = True\n",
    "    if gpu_info.free >= 24:\n",
    "        args.gradient_checkpointing = False\n",
    "    else:\n",
    "        args.gradient_accumulation_steps = 1\n",
    "        args.gradient_checkpointing = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_dreambooth import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   --pretrained_model_name_or_path=./realisticVisionV13.LtFu.safetensors \\\n",
    "#   --instance_data_dir=/mnt/vol1/inputs/alvan-nee-cropped \\\n",
    "#   --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "#   --class_data_dir=./class_dog --output_dir=./mydog_real \\\n",
    "#   --instance_prompt=\"a photo of my dog\" --class_prompt=\"a photo of a dog\" \\\n",
    "#   --resolution=512 --models_dir=/mnt/vol1/models/Stable-diffusion/mydog_real/ \\\n",
    "#   --auto_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/scripts/gcp-stable-diffusion-build-deploy/Training'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_args = [\"--pretrained_model_name_or_path=./realisticVisionV13.LtFu.safetensors\",\n",
    "  \"--instance_data_dir=/mnt/vol1/inputs/alvan-nee-cropped\",\n",
    "  \"--with_prior_preservation\", \"--prior_loss_weight=1.0\",\n",
    "  \"--class_data_dir=./class_dog\", \"--output_dir=./mydog_real\",\n",
    "  \"--instance_prompt='a photo of my dog'\",\n",
    "  \"--class_prompt='a photo of a dog'\",\n",
    "  \"--resolution=512\", \"--models_dir=/mnt/vol1/models/Stable-diffusion/mydog_real/\",\n",
    "  \"--auto_guess\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/scripts/gcp-stable-diffusion-build-deploy/Training'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./mydog_real'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/vol1/models/Stable-diffusion/mydog_real/'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.models_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./mydog_real/checkpoint-100',\n",
       " './mydog_real/checkpoint-300',\n",
       " './mydog_real/checkpoint-500',\n",
       " './mydog_real/checkpoint-200',\n",
       " './mydog_real/checkpoint-400']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dirs = glob(os.path.join(args.output_dir, 'checkpoint-*'))\n",
    "checkpoint_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python convert_diffusers_to_original_stable_diffusion.py --model_path=./mydog_real/checkpoint-500 --checkpoint_path=/mnt/vol1/models/Stable-diffusion/mydog_real/./mydog_real-500.safetensors --use_safetensors\n",
      "Reshaping encoder.mid.attn_1.q.weight for SD format\n",
      "Reshaping encoder.mid.attn_1.k.weight for SD format\n",
      "Reshaping encoder.mid.attn_1.v.weight for SD format\n",
      "Reshaping encoder.mid.attn_1.proj_out.weight for SD format\n",
      "Reshaping decoder.mid.attn_1.q.weight for SD format\n",
      "Reshaping decoder.mid.attn_1.k.weight for SD format\n",
      "Reshaping decoder.mid.attn_1.v.weight for SD format\n",
      "Reshaping decoder.mid.attn_1.proj_out.weight for SD format\n",
      "python convert_diffusers_to_original_stable_diffusion.py --model_path=./mydog_real/checkpoint-200 --checkpoint_path=/mnt/vol1/models/Stable-diffusion/mydog_real/./mydog_real-200.safetensors --use_safetensors\n",
      "Reshaping encoder.mid.attn_1.q.weight for SD format\n",
      "Reshaping encoder.mid.attn_1.k.weight for SD format\n",
      "Reshaping encoder.mid.attn_1.v.weight for SD format\n",
      "Reshaping encoder.mid.attn_1.proj_out.weight for SD format\n",
      "Reshaping decoder.mid.attn_1.q.weight for SD format\n",
      "Reshaping decoder.mid.attn_1.k.weight for SD format\n",
      "Reshaping decoder.mid.attn_1.v.weight for SD format\n",
      "Reshaping decoder.mid.attn_1.proj_out.weight for SD format\n",
      "python convert_diffusers_to_original_stable_diffusion.py --model_path=./mydog_real/checkpoint-400 --checkpoint_path=/mnt/vol1/models/Stable-diffusion/mydog_real/./mydog_real-400.safetensors --use_safetensors\n",
      "Reshaping encoder.mid.attn_1.q.weight for SD format\n",
      "Reshaping encoder.mid.attn_1.k.weight for SD format\n",
      "Reshaping encoder.mid.attn_1.v.weight for SD format\n",
      "Reshaping encoder.mid.attn_1.proj_out.weight for SD format\n",
      "Reshaping decoder.mid.attn_1.q.weight for SD format\n",
      "Reshaping decoder.mid.attn_1.k.weight for SD format\n",
      "Reshaping decoder.mid.attn_1.v.weight for SD format\n",
      "Reshaping decoder.mid.attn_1.proj_out.weight for SD format\n"
     ]
    }
   ],
   "source": [
    "for dir in checkpoint_dirs[2:]:\n",
    "    n_steps = os.path.basename(dir).split('-')[-1]\n",
    "    if os.path.isdir(os.path.join(args.output_dir, \"vae\")) and not os.path.isdir(os.path.join(dir, \"vae\")):\n",
    "        print(\"cp -rp {}/vae {}\".format(args.output_dir, dir))\n",
    "        os.system(\"cp -rp {}/vae {}\".format(args.output_dir, dir))\n",
    "    if os.path.isdir(os.path.join(args.output_dir, \"unet\")) and not os.path.isdir(os.path.join(dir, \"unet\")):\n",
    "        print(\"cp -rp {}/unet {}\".format(args.output_dir, dir))\n",
    "        os.system(\"cp -rp {}/unet {}\".format(args.output_dir, dir))\n",
    "        \n",
    "    print(\"python convert_diffusers_to_original_stable_diffusion.py --model_path={} --checkpoint_path={} --use_safetensors\".format(dir, os.path.join(args.models_dir, args.output_dir + f'-{n_steps}.safetensors')))\n",
    "    os.system(\"python convert_diffusers_to_original_stable_diffusion.py --model_path={} --checkpoint_path={} --use_safetensors\".format(dir, os.path.join(args.models_dir, args.output_dir + f'-{n_steps}.safetensors')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(pretrained_model_name_or_path='./realisticVisionV13.LtFu.safetensors', models_dir='/mnt/vol1/models/Stable-diffusion/mydog_real/', revision=None, tokenizer_name=None, instance_data_dir='/mnt/vol1/inputs/alvan-nee-cropped', class_data_dir='./class_dog', instance_prompt=\"'a photo of my dog'\", class_prompt=\"'a photo of a dog'\", with_prior_preservation=True, prior_loss_weight=1.0, num_class_images=100, output_dir='./mydog_real', seed=None, resolution=512, center_crop=False, train_text_encoder=False, train_batch_size=4, sample_batch_size=4, num_train_epochs=1, max_train_steps=None, checkpointing_steps=500, checkpoints_total_limit=None, resume_from_checkpoint=None, gradient_accumulation_steps=1, gradient_checkpointing=False, learning_rate=5e-06, scale_lr=False, lr_scheduler='constant', lr_warmup_steps=500, lr_num_cycles=1, lr_power=1.0, use_8bit_adam=False, dataloader_num_workers=0, adam_beta1=0.9, adam_beta2=0.999, adam_weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, push_to_hub=False, hub_token=None, hub_model_id=None, logging_dir='logs', allow_tf32=False, report_to='tensorboard', validation_prompt=None, num_validation_images=4, validation_steps=100, mixed_precision=None, prior_generation_precision=None, local_rank=-1, enable_xformers_memory_efficient_attention=False, set_grads_to_none=False, auto_guess=True)\n"
     ]
    }
   ],
   "source": [
    "pprint(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adam_beta1': 0.9,\n",
      " 'adam_beta2': 0.999,\n",
      " 'adam_epsilon': 1e-08,\n",
      " 'adam_weight_decay': 0.01,\n",
      " 'allow_tf32': False,\n",
      " 'auto_guess': True,\n",
      " 'center_crop': False,\n",
      " 'checkpointing_steps': 500,\n",
      " 'checkpoints_total_limit': None,\n",
      " 'class_data_dir': './class_dog',\n",
      " 'class_prompt': \"'a photo of a dog'\",\n",
      " 'dataloader_num_workers': 0,\n",
      " 'enable_xformers_memory_efficient_attention': False,\n",
      " 'gradient_accumulation_steps': 1,\n",
      " 'gradient_checkpointing': False,\n",
      " 'hub_model_id': None,\n",
      " 'hub_token': None,\n",
      " 'instance_data_dir': '/mnt/vol1/inputs/alvan-nee-cropped',\n",
      " 'instance_prompt': \"'a photo of my dog'\",\n",
      " 'learning_rate': 5e-06,\n",
      " 'local_rank': -1,\n",
      " 'logging_dir': 'logs',\n",
      " 'lr_num_cycles': 1,\n",
      " 'lr_power': 1.0,\n",
      " 'lr_scheduler': 'constant',\n",
      " 'lr_warmup_steps': 500,\n",
      " 'max_grad_norm': 1.0,\n",
      " 'max_train_steps': None,\n",
      " 'mixed_precision': None,\n",
      " 'models_dir': '/mnt/vol1/models/Stable-diffusion/mydog_real/',\n",
      " 'num_class_images': 100,\n",
      " 'num_train_epochs': 1,\n",
      " 'num_validation_images': 4,\n",
      " 'output_dir': './mydog_real',\n",
      " 'pretrained_model_name_or_path': './realisticVisionV13.LtFu.safetensors',\n",
      " 'prior_generation_precision': None,\n",
      " 'prior_loss_weight': 1.0,\n",
      " 'push_to_hub': False,\n",
      " 'report_to': 'tensorboard',\n",
      " 'resolution': 512,\n",
      " 'resume_from_checkpoint': None,\n",
      " 'revision': None,\n",
      " 'sample_batch_size': 4,\n",
      " 'scale_lr': False,\n",
      " 'seed': None,\n",
      " 'set_grads_to_none': False,\n",
      " 'tokenizer_name': None,\n",
      " 'train_batch_size': 4,\n",
      " 'train_text_encoder': False,\n",
      " 'use_8bit_adam': False,\n",
      " 'validation_prompt': None,\n",
      " 'validation_steps': 100,\n",
      " 'with_prior_preservation': True}\n"
     ]
    }
   ],
   "source": [
    "pprint(vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
